{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bkneussl/Assignments/blob/main/assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hide\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "from fastai.vision.all import *\n",
        "from fastbook import *\n",
        "fastbook.setup_book()"
      ],
      "metadata": {
        "id": "HhmY7I5M8VJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e6cc888-a117-4097-fc20-d6f16e56be58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Artificial Neural Networks\n",
        "\n",
        "Please read the introdcution of neuronal networks of the book *Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow*, p. 299-316.\n",
        "\n",
        "######**Why have neural networks, even though they were invented early on, only now caught on?**\n",
        "\n",
        "Neural networks, or artificial neural networks (ANNs), were initially developed in the 1940s, but their early successes did not lead to widespread adoption due to limitations in computing power and training algorithms. In the 1960s, funding for ANNs decreased, and other machine learning techniques, such as support vector machines, gained prominence. However, ANNs experienced a revival in the 1980s due to new architectures and improved training techniques. Despite this, progress was slow, and other machine learning techniques continued to outperform ANNs in many applications.\n",
        "\n",
        "It is only recently that ANNs have caught on, due to several factors. First, there is now a vast amount of data available to train neural networks, and ANNs often outperform other machine learning techniques on complex problems. Second, there has been a tremendous increase in computing power since the 1990s, making it possible to train large neural networks in a reasonable amount of time. Third, training algorithms have been improved, making them more effective. Fourth, some theoretical limitations of ANNs, such as the risk of getting stuck in local optima during training, have turned out to be benign in practice. Finally, ANNs have entered a virtuous circle of funding and progress, with amazing products based on ANNs regularly making headline news, attracting attention and funding, and leading to even more progress and impressive products.\n",
        "\n",
        "\n",
        "######**What is a percepton and a threshold logic unit (TLU)? Try to define a linear function and a step function of your choice, use some values of your choice and explain what might be the result of the percepton. (maybe using max. two TLU's)**\n",
        "\n",
        "Linear function: A linear function is a mathematical function that produces a straight line when plotted on a graph. An example of a linear function is y = 2x + 1. This function has a slope of 2 and a y-intercept of 1. If we feed this function into a perceptron with a single TLU (Threshold Logic Unit), the perceptron will learn to adjust its weights such that it can draw a line that approximates this function. For example, if the perceptron's weights are initially set to w1 = 0.5 and w2 = 0.5, and the bias is set to b = -1, the perceptron will learn to adjust its weights and bias to produce an output of y = 2x + 1 when given input values of x. The perceptron might adjust its weights and bias to w1 = 2, w2 = 0, and b = 1, which would produce the same output as the original function.\n",
        "\n",
        "Step function: A step function is a mathematical function that produces a constant output for any input value above a certain threshold, and a different constant output for any input value below that threshold. An example of a step function is the Heaviside step function, which is defined as follows: \n",
        " \n",
        "H(x) = {\n",
        "           0 for x < 0,\n",
        "           1/2 for x = 0,\n",
        "           1 for x > 0\n",
        "         }\n",
        "\n",
        "If we feed this function into a perceptron with a single TLU, the perceptron will learn to adjust its weights and bias such that it can approximate the function. For example, if the perceptron's weights are initially set to w1 = 0.5 and w2 = 0.5, and the bias is set to b = -0.5, the perceptron will learn to adjust its weights and bias to produce an output of H(x) when given input values of x. The perceptron might adjust its weights and bias to w1 = 0, w2 = 1, and b = -0.5, which would produce the same output as the Heaviside step function.\n",
        "\n",
        "\n",
        "\n",
        "######**What is a fully connected layer and a output layer?**\n",
        "\n",
        "A fully connected layer, or a dense layer is a perceptron that is composed of one or more TLUs organized in a single layer, where every TLU is connected to every input.\n",
        "\n",
        "\n",
        "\n",
        "######**Why can we easily combine the equations of multiple instances into a fully connected layer?**\n",
        "\n",
        "We can easily combine the equations of multiple instances into a fully connected layer because each neuron in a fully connected layer is connected to all the neurons in the previous layer. This means that the output of each neuron in the previous layer serves as an input to every neuron in the fully connected layer. Therefore, we can combine the equations of multiple instances by representing the inputs to the fully connected layer as a matrix and the weights of the layer as a matrix. \n",
        "\n",
        "\n",
        "\n",
        "######**What problem did Marvin Minsky and Seymour Paper highlight that perceptrons could not solve? What is a possible solution?**\n",
        "\n",
        "In their 1969 monograph Perceptrons, Marvin Minsky and Seymour Papert high‐ lighted a number of serious weaknesses of perceptrons—in particular, the fact that they are incapable of solving some trivial problems like XOR classification. The limitations of perceptrons can be eliminated by stacking multiple perceptrons.\n",
        "\n",
        "\n",
        "\n",
        "######**What is a deep neuronal network? What are hidden layers? What means feedforward neural network (FNN).**\n",
        "\n",
        "A deep neural network (DNN) is a type of artificial neural network (ANN) that contains multiple hidden layers between the input and output layers. In contrast, a shallow neural network contains only a single hidden layer or none at all.\n",
        "\n",
        "Each hidden layer in a DNN consists of one or more neurons that perform a nonlinear transformation of the inputs received from the previous layer. These transformations are learned during the training process, where the network is fed a set of input-output pairs and adjusts its weights and biases to minimize the difference between its predicted outputs and the actual outputs.\n",
        "\n",
        "A feedforward neural network (FNN) is a type of artificial neural network where the information flows in only one direction, forward from the input layer, through the hidden layers, and to the output layer. There are no feedback loops where the output of the network is fed back into itself.\n",
        "\n",
        "\n",
        "\n",
        "######**Try to explain how backpropagation works!** (In Addition, you can have a look to the following example, which tries manually to compute the backprogation of a simple linear network. https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ OR you can also read through the Google Colab [04_mnist_basics.ipynb](https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb#scrollTo=t1DK6o-gckCy))\n",
        "\n",
        "\n",
        "\n",
        "1. First, the network processes a mini-batch of input instances at once.\n",
        "2. The inputs are fed forward through the network, starting with the input layer and passing through each hidden layer until the output layer is reached. This forward pass calculates the output of each neuron for each instance in the mini-batch.\n",
        "3. The network's output is compared to the desired output using a loss function, which calculates the error.\n",
        "4. Using the chain rule of calculus, the algorithm calculates how much each connection in the output layer contributed to the error. This calculation is done for each instance in the mini-batch.\n",
        "5. The error contributions are then backpropagated to the layer before the output layer, and the algorithm calculates how much each connection in this layer contributed to the error.\n",
        "6. This backpropagation process is repeated for each layer until the input layer is reached.\n",
        "7. With the error gradients calculated for each connection weight and bias in the network, the algorithm performs a gradient descent step to adjust each weight and bias to minimize the error for the current mini-batch.\n",
        "8. Steps 2-7 are repeated for each mini-batch in the training set, multiple times (epochs), until the network's performance on a validation set is satisfactory.\n",
        "\n",
        "\n",
        "\n",
        "######**Why do we need activation functions, wouldn't it be easier just using linear functions?**\n",
        "\n",
        "A linear function has a constant derivative, which means that it does not provide useful information for the backpropagation algorithm to adjust the weights of the network.\n",
        "\n",
        "\n",
        "\n",
        "## Ideas for the learning portfolio: \n",
        "\n",
        "1) For example, you could train a single TLU to classify iris flowers based on petal length and width in the !!!pyTorch!! environment.\n",
        "\n",
        "2) You could add to our king county housepricing ML project a neuronal network and compare it to the other models. "
      ],
      "metadata": {
        "id": "_Rdj49uwjuoU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "51-m1sNKkhqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()"
      ],
      "metadata": {
        "id": "4tF8YDV3T91n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A traditional approach: training a digit classifier and learning pyTorch tensors.\n",
        "\n",
        "For this assignment, I ask you to read the Google Colab [04_mnist_basics.ipynb](https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb#scrollTo=t1DK6o-gckCy) to the beginning of the chapter *Stochastic Gradient Descent (SGD)*. \n",
        "\n",
        "First, try to summarize what we know about pyTorch tensors by trying to predict whether we have a 1 or a 7 in the MNIST dataset using a traditional rule-based programming approach. Therefore use pyTorch tensors for the entire tasks and fulfill the following steps:\n",
        "\n",
        "1) Randomly split the MNIST dataset (1 and 7) into a training dataset and a test dataset in a ratio of 80:20.\n",
        "\n",
        "2) Instead of using an optimal 1 or 7 with the mean over the training dataset, try to calculate the sum of the distances to all instances in the training set for each instance in the test dataset. You can use the L2 norm. \n",
        "\n",
        "3) For each instance in the test set, decide if it is a 1 or 7 and calculate the precision.\n",
        "\n",
        "Do we get a similar good result?\n"
      ],
      "metadata": {
        "id": "h6OwXNEeed93"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hrrgv9OVebAH"
      },
      "outputs": [],
      "source": [
        "# YOUR TASK"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient Descent (SGD)\n",
        "\n",
        "For this exercise I ask you to read the chapter Stochastic Gradient Descent (SGD) from the Google Colab 04_mnist_basics.ipynb in paralell. The chapter starts with a single TLU, compare p. 304 in \"Hands on Machine Learning\". Go through all 7 steps which are an easy example of how Stochastic Gradient Descent works.\n",
        "\n",
        "Our goal is to train a single TLU, which can decide if one number is larger then the other one. Therefore we create 100 random pairs with pyTorch and create a target vector which is eather 1 or 0.\n"
      ],
      "metadata": {
        "id": "ETcE9B9rdcEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((100, 2))\n",
        "y = torch.where(x[:,0] > x[:,1], 1.0, 0.0)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "17qLyDnbpSbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to create a function f that is a single TLU, meaning that it summarizes x with weights a, b, c:\n",
        "\n",
        "$ax_0+bx_1+c$\n",
        "\n",
        "In Addition we are using a *sigmoid()* function as step function.\n",
        "\n",
        "$f = \\text{sigmoid}(ax_0+bx_1+c)$"
      ],
      "metadata": {
        "id": "z267w4G48rxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x, params):\n",
        "    a,b,c = params\n",
        "    return #YOUR TASK \n",
        "\n",
        "print(f(x, [3,-2,1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_NvBnCGoLPx",
        "outputId": "f15684b2-c773-47ac-a705-db5337e3ec08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0138, 0.9517, 0.9690, 0.5570, 0.9972, 0.9988, 0.8296, 0.8440, 0.9849, 0.2474, 0.0599, 0.0372, 0.9726, 0.9878, 0.4036, 0.3938, 0.0367, 0.9965, 0.7762, 0.9998, 0.0793, 0.9431, 0.5434, 0.9998,\n",
            "        0.9779, 0.0594, 0.4891, 0.0048, 0.4292, 0.6232, 0.6732, 0.0202, 0.4517, 0.9995, 0.2007, 0.2726, 0.8806, 0.0339, 0.7862, 0.6669, 0.0659, 0.5742, 0.9866, 0.3536, 0.1037, 0.9929, 0.4275, 0.3508,\n",
            "        0.4192, 0.1788])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to our TLU function, we need a loss function. Your task is to implement a absolute difference loss function, $∑|x_i-y_i|$, which counts the number of wrong guesses."
      ],
      "metadata": {
        "id": "UBiKkGKx-jVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mae(preds, targets): return #YOUR TASK"
      ],
      "metadata": {
        "id": "cwzyy281wI7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to train your single TLU with the absolute difference loss function, use the following code. Choose an appropriate step weight `lr` and try to explain what is happing in each line."
      ],
      "metadata": {
        "id": "eGVNErmbvFxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1\n",
        "params = torch.randn(3).requires_grad_()\n",
        "\n",
        "def apply_step(params, prn=True):\n",
        "    preds = f(x, params)\n",
        "    loss = mae(preds, y)\n",
        "    loss.backward()\n",
        "    params.data -= lr * params.grad.data\n",
        "    params.grad = None\n",
        "    if prn: print(params);print(loss.item())\n",
        "    return preds\n",
        "\n",
        "\n",
        "for i in range(50): apply_step(params)"
      ],
      "metadata": {
        "id": "EB5TYTNmyO3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a line of code that counts the number of wrong predictions, rounding your predictions with *round()*."
      ],
      "metadata": {
        "id": "h5_LNc1o_o2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = f(x, params)\n",
        "#YOUR TASK"
      ],
      "metadata": {
        "id": "EEUhyhyDxwMQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}