{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bkneussl/Assignments/blob/main/assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theory\n",
        "\n",
        "In the following assignment, your task is to complete the MNIST Basics chapter. It is best to repeat everything from last week and try to answer the following questions. Afterwards you have to summarize the learned facts with two programming tasks.\n",
        "\n",
        "**What is \"torch.cat()\" and \".view(-1, 28*28)\" doing in the beginning of the \"The MNIST Loss Function\" chapter?**\n",
        "\n",
        "\n",
        "The torch.cat function combines two tensors by appending one to the other. On the other hand, torch.view can modify the dimensions of tensors. In the context of the example provided, where we have a tensor of shape (N, 28, 28), using view with a parameter of (-1, 2828) reduces the number of dimensions by one, resulting in a tensor of shape (N, 2828).\n",
        "\n",
        "**Can you draw the neuronal network, which is manually trained in chapter \"The MNIST Loss Function\"?**\n"
      ],
      "metadata": {
        "id": "iIBgQ5f43H6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is it not possible to use the accuracy as loss function?**\n",
        "\n",
        "The accuracy of the prediction only changes when the model predicts a 3 and the correct answer is 7, or vice versa. If we make small adjustments to the input data, the prediction may not change at all because the gradient is almost zero throughout most of the input space.\n",
        "\n",
        "\n",
        "\n",
        "**What is the defined `mnist_loss` function doing? **\n",
        "\n",
        "\n",
        "```\n",
        "def mnist_loss(predictions, targets):\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "```\n",
        "\n",
        "\n",
        "The purpose of this function is to calculate the distance between each prediction and the correct output value, which is either 0 or 1. The function computes the distance from 1 if the correct output is 1, and from 0 if the correct output is 0. The resulting distances are then averaged across all predictions. In essence, this function serves as a type of ternary operator, operating on PyTorch Tensors.\n",
        "\n",
        "**Why do we need additionaly the sigmoid() function? What is it technically in our TLU?**\n",
        "\n",
        "\n",
        "The Sigmoid function is utilized in the MNIST Loss function to constrain the values passed to it between 0 and 1. Specifically, it is used as the activation function of a Threshold Linear Unit (TLU) which enables the approximation of non-linear functions by introducing non-linearity into the network.\n",
        "\n",
        "**Again, what are mini batches, why are we using them and why should they be shuffeld?** \n",
        "\n",
        "\n",
        "Mini-batches are smaller subsets of the training data that are utilized to update the model's parameters during training. Rather than using the entire dataset to perform a single update, the dataset is partitioned into smaller, equally-sized batches. These batches are processed, and the model's parameters are updated based on the average loss computed over the mini-batch.\n",
        "\n",
        "Using mini-batches has several benefits over using the entire dataset. Firstly, it is computationally more efficient on GPUs because parallelization can be utilized to process multiple batches simultaneously. Secondly, mini-batches are a compromise between computing the loss function for a single instance and for every instance, thus speeding up the training process. Selecting an appropriate batch size is a critical decision to make as a deep learning practitioner in order to efficiently and effectively train the model.\n",
        "\n",
        "It is crucial to shuffle the mini-batches during training to ensure that each mini-batch is a representative sample of the entire dataset. If the data is not shuffled, the model may encounter similar examples in each mini-batch, resulting in poor generalization and overfitting. Shuffling the mini-batches ensures that the model is exposed to a diverse range of examples in each iteration, aiding in better generalization."
      ],
      "metadata": {
        "id": "Bj3r6KrlTREG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Part\n",
        "\n",
        "Try to understand all parts of the code needed to manually train a single TLU/Perceptron, so use and copy all parts of the code from \"First Try: Pixel Similarity\" to the \"Putting it all together\" chapter. In the second step, use an optimizer, a second layer, and a ReLU as a hidden activation function to train a simple neural network. When copying the code, think carefully about what you really need and how you can summarize it as compactly as possible. (Probably each attempt requires about 15 lines of code.)"
      ],
      "metadata": {
        "id": "aoQq7z5D3XXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TxwyNuzj3DYu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "6bd459ff-a4a0-4c78-e93f-73b3c007c6b6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='3219456' class='' max='3214948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.14% [3219456/3214948 00:00&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: Valid Loss: 544.1332397460938, Valid Acc: 0.4996914863586426\n"
          ]
        }
      ],
      "source": [
        "#YOUR TASK: Manually train a single layer perceptron without using an optimizer.\n",
        "\n",
        "# Load data\n",
        "from fastai.vision.all import *\n",
        "path = untar_data(URLs.MNIST_SAMPLE)\n",
        "threes = (path/'train'/'3').ls().sorted()\n",
        "sevens = (path/'train'/'7').ls().sorted()\n",
        "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
        "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
        "stacked_threes = torch.stack(three_tensors).float()/255\n",
        "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
        "\n",
        "# Prepare data for training and validation\n",
        "train_x = torch.cat([stacked_threes[:500], stacked_sevens[:500]]).view(-1, 28*28)\n",
        "train_y = tensor([1]*500 + [0]*500).unsqueeze(1)\n",
        "valid_x = torch.cat([stacked_threes[500:], stacked_sevens[500:]]).view(-1, 28*28)\n",
        "valid_y = tensor([1]*(len(threes)-500) + [0]*(len(sevens)-500)).unsqueeze(1)\n",
        "train_dset = list(zip(train_x,train_y))\n",
        "valid_dset = list(zip(valid_x,valid_y))\n",
        "\n",
        "# Initialize weights and bias\n",
        "def init_params(size, std=1.0):\n",
        "    return (torch.randn(size)*std).requires_grad_()\n",
        "weights = init_params((28*28,1))\n",
        "bias = init_params(1)\n",
        "\n",
        "# Define model\n",
        "def linear1(xb):\n",
        "    return xb@weights + bias\n",
        "\n",
        "# Define loss function\n",
        "def mnist_loss(predictions, targets):\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "\n",
        "# Calculate accuracy\n",
        "def accuracy(preds, targets):\n",
        "    preds = preds.sigmoid()\n",
        "    return ((preds > 0.5) == targets).float().mean()\n",
        "\n",
        "# Train model\n",
        "lr = 1.\n",
        "for epoch in range(20):\n",
        "    # Training phase\n",
        "    for xb,yb in train_dset:\n",
        "        preds = linear1(xb)\n",
        "        loss = mnist_loss(preds, yb)\n",
        "        loss.backward()\n",
        "        weights.data -= weights.grad*lr\n",
        "        bias.data -= bias.grad*lr\n",
        "        weights.grad.zero_()\n",
        "        bias.grad.zero_()\n",
        "\n",
        "# Validation phase\n",
        "valid_preds = [linear1(xb) for xb,yb in valid_dset]\n",
        "valid_loss = mnist_loss(torch.cat(valid_preds), valid_y)\n",
        "valid_acc = accuracy(torch.cat(valid_preds), valid_y)\n",
        "print(f\"Epoch {epoch}: Valid Loss: {valid_loss}, Valid Acc: {valid_acc}\")\n",
        "from fastai.vision.all import *\n",
        "from fastai.vision.core import *\n",
        "from fastai.vision.data import *\n",
        "from fastai.vision.learner import *\n",
        "from fastai.vision.models import *\n",
        "from fastai.metrics import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR TASK: Train a simple two-layer neural network (two perceptrons + hidden activation function) with built-in functions and an optimizer.\n",
        "\n",
        "# Load data\n",
        "path = untar_data(URLs.MNIST_SAMPLE)\n",
        "train_items = get_image_files(path/'train')\n",
        "train_labels = [1 if str(item).split('/')[-2] == '3' else 0 for item in train_items]\n",
        "train_dset = [(PILImage.create(item), label) for item, label in zip(train_items, train_labels)]\n",
        "dls = DataLoader(train_dset, batch_size=256)\n",
        "\n",
        "# Define model\n",
        "class TwoLayerNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(28*28, 50)\n",
        "        self.layer2 = nn.Linear(50, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model and optimizer\n",
        "model = TwoLayerNet()\n",
        "opt = SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Define loss function and metric\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "metrics = accuracy_multi\n",
        "\n",
        "# Train model\n",
        "learn = Learner(dls, model, opt_func=opt, loss_func=loss_func, metrics=metrics)\n",
        "learn.fit(10)\n",
        "\n"
      ],
      "metadata": {
        "id": "UGsLRFtMbyRZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "outputId": "1ddbc16c-7bb4-4ecb-de16-ed0c4b9ad48e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-41668fcee34b>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt, start_epoch)\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0mcbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSkipToEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_cbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mreset_opt\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwd\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_hypers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36mcreate_opt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd_bn_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bn_bias_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'do_wd'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Optimizer' object is not callable"
          ]
        }
      ]
    }
  ]
}